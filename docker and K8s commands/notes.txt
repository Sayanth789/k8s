Node and Pod -- 
-------------------------------------------------

Node is a virtual or physical machine

Pod is basic unit or smallest unit.

Provides abstraction over container.

We interact with the k8s 

Usually a pod is meant to be run as 1 application per Pod.

Each pod gets its own IP address.


service is a static or permanent IP Address that can be attached to the pod.

Licecycle of Pod and sercice is not connected.

ingress : To make the connection secure.


======================================================

Configmap and Secret 
-----------------------

External configuration of the application.

Secret: To store secret data. Stored in base64 format.

---------------------------------------------------------------------

DataStorage 
==================

Volume : Attaches a physcal storage, Can be remote or outside of the k8s cluster.

             Deployment and Statelessness.
===========-------------=============--------------

** Node Process

Worker server, each node has mulitple pode in it.


3 processes must be installed on every node 

Worker Nodes do the actual work.


********            *********            *********          **********
Master Process 

Cluster the gateway.

Acts as a gatekeeper for the authentication!.

minikube 

Creates Virtual Box on the lap 

Node runs in that Virtual Box 

1 Node k8s cluster 

for testing purposes 

88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888

3 Parts of the config file:

1) Metadata 

2) specification 

3) status : Automatically made.

-- self healing feature.

check how to use mongo in docker .


Namespace :

* Resources grouped Namespace

* Conflits: Many teams, same application.

*  Resources Sharing: Staging and Development 

* Blue/Green Development

* Access and Resources Limit on Namespaces.

----------------------------------------------

You can't access most Resources from another Namespaces.

........kubens!?????????????????????????????????????????


::::::  Helm ::::::

Package manager for K8s 

To Package YAML files, and distribute them in public and private repositories.

ElsticSearch , MySQL , MongoDB and monitoring apps like Prometheaus all have helm charts.


    *_ Templating Engine.
-------------------------

Helm Chart Structure 

Directory Structure

mychart/              Top level folder name of chart
  Chart.yaml 
  values.yaml         Chart.yaml -> metadata info about chart   
  charts/
  templates/           values.yaml -> values for the template files.     
  ...

  Chart folder has chart dependecies.
  template folder : where template fiels are stored.


Values injection into template files.
-----------------------------------
values.yaml 

ImageName: myapp 
port: 8080 
varsion: 1.0.0 

k8s volumes: 

How to persist data in k8s using volumes???.

The need for volumes.

No data persist out of the box, we need to configure that.

  persistent volume claim
--------------------------

ElsticNet???

storage Class: 
SC provisions Persistent Volumes dynamically.


  eg: : : : : : : 


apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata: 
  name: storage-class-name
provisioner: kubernetes.io/aws-ebs 
parameters: 
  type: io1 
  iopsPerGB: "10"
  fsType: ext4

each storage class has its own provisioner.

Internal provisioner:  kubernetes.io/

Another abstraction level 

abstracts underlying storage provider

parameters for that storage.

PVC (Rquested by persistent volume claim)

apiVersion: v1 
kind: PersistentVolumeClaim 
metadata: 
  name: mypvc
spec: 
  accessMode: 
  - ReadWriteOnce 
  sources: 
  requests: 
    storage: 100Gi 
  storageClassName: storage-class-name    


StatefullSet 

What is StatefulSet:::::

*** Used for stateful applications.

databases are eg: MySQL, ElsticSearch, mongodb 


Or any app that stores data 
Stateless apps don't keep record of state.
Each request is completely new.

* Stateless apps are deployed using deply components.

* stateful apps are deployed using stateful set.

* Both manages pod based on container specification.

confiure storage the same way.

selector : which pods to forward request to?

which port to forward request to?

targetPort: : : 

3 types of service
clusterIP    NodePort   LoadBalancer



    Labels 

key-value pairs used to identify, describe and group related sets of objects or resources

selectors; 
  *  use labels to filter or select objects.

PATTERNS 
**   ** **   ** 

sidecar 

adapter 

ambassador 

Networking 
---------------------
multiple containers

Workloads 
is an app running on the K8s 
  Pod - set of running containers 

  ReplicaSet -- 
  Deployment -- 
  StatefullSet
  DaemonSet 
    -- provide node-local facilities, such as storage driver or network plugin.
  Tasks that runs to completion 
   Job 
   CronJob 


ReplicaSet 
  Primary methd of mananging pod replcas and their lifecycle to provide self-healing capabilities.
  Their job is always to ensure the desired number of pods are running.

  While we can create ReplicaSets, the recommended way is to create Deployments

  Self Healing : create maually 
===========================================================================

k apply -f [definition.yml]   Crete ReplicaSet
k get rs  : List Replicaset 
k describe rs [rsName]   -- Get info 
k delete -f [definition.yml]   Delete a ReplicaSet
k delete rs [rsName]   Same but using the ReplicaSet name.

===========================================================================

pods don't self heal. 

* ca't update or rollback 

Deployment can :
* It make ReplicaSet in the background.
* Dont interact with the ReplicaSet Directly.


k create deploy [deplymentName] --image=busybox --replicas=3 --port=80 

k apply -f [definition.yml]          :  Create a Deployment

k get deploy          :    List deployment 
 
k describe deply [deplymentName] :   Get info 

k get rs :                     List ReplicaSet

k delete -f [definition.yaml]  : Delete a deployment 

k delete deply [deplymentName]   same but using the deployment name 


------------------------------------
DaemonSet
------------------------------------

Ensures all Nodes (or subset) runs an instance of a Pod 

Scheduled by the Scheduler controller and run by the deamon controller 

As nodes are added to the cluster , Pods are added to them.


Typical uses: 
*  Running a cluster storage daemon 

*  Running a logs collection  daemon on every node 

*  Running a node monitoring daemon on every node.


k apply -f [definition.yml]   = Creates a DaemonSet

k get ds  List DaemonSet

k describe ds [rsName]   Get info 

k delete -f [definition.yaml]   Deletes 

k delete ds [rsName]   Using the DaemonSet name. 


--------------------------------------------------------------------------------------------


Jos 
Workload for short lived tasks 
Creates one or more pods and ensure that a specified number of them successfully terminate 
As pods successfully complete, the job tracks the successful completions 
When a specified number of successful  completion is reached, the job compltes
By default , jobs with more than 1 pod will create them one after the other. To create them at the same time, add parallelism 

k create job [jobName]  --image=busybox  The imperative way 
k apply -f [definition.yml]   : Create a job 
k get job : List jobs 
k describe job [jobName] : Get info 
k delete -f [definition.yml]   Delete job 
k delete job [jobName]

======================____________________________===============================____________________________
---------------------------------------------------------------------------------------------------------------------------------------
CronJob
 * Extension of Job

*******************---------------------------------((((((((((((((((((((((((((()))))))))))))))))))))))))))********************************_____________________________________



Rolling Update 

Maxsurge 

* Max: number of pods that can be created over the desired number of pods 

* Value or percentage 

* maxUnavailable 
 Max: number of pods that can be unavilable during the update 

 default both set to 25% 

Blue Green deployment 

Doenst solve the new database schema problem entirely 

We need to over provision the cluster size 


k create -f hello-dep-v2.yaml ????



clusterIP:
default service 

Visibility 
Cluster Internal

Port 
 The pirt the service is listening to.

 TargetPort : Redirecting to the port the pofs are listening to 



k expose po [podName] --port=80 --target-port=8080 --name=frontend

k expose deploy [deployName] --port=80 --target-port=8080 

etc....


NodePort
Extend the ClisterIP service 

Visibility Internal and External

 NodePort The port service is listening to extenrally 

 Statically defined or dynamically taken from a range between 3000-32767 
Port 
----------------------------------
Services: 
----------------------------------

another type of k8s object 
pod IPs are unreliable but services IPs are
Durable : 
* Static : IP address 
* Static DNS name 
[servicename] [Namespace].svc.cluster.local 
* Services are ways to access pods 
target pods using selectors 

Layer4Load baliacing 

Opearting at the transport level (TCP)
Unable to make decision based on the content 
simple algiorithms such as round-robin routing 

Layer7 LoadBalancing 

Operates at the appliaction level (HTTP, SMTP, etc)
Able to make decisions based on the actual content of each message 
More intelligent load balancing decisions and content optimizations 
 *routing rules 
 * 
=============================================================================================
Storage and Persistence 
----------____________------------_____________\


we don't save data in containers

Non persistent data 
 * Locally on a writtable layer 
  *  It's the default , just write to the filesystem 
  * When containers are destroyed , so the data inside them 
  * We need to store the data outside containers

Volume let store data outside the containers

Volumes let container store data into the extrnal storage systems 

Vendors crate plugins for their storage systems  according to the Container Storage Interface 

two ways to cr3eate volumes 

            Static and Dynamic ways 
Persitent  volume represent a storage resource 
Cluster wide 

Provisioned by an administrator 

 Persistence Volume Claim 
  A one - to - one mapping to a persistent volume 

One or more pods can  use a Persitent Volume Claim 

Can be consumed by any of the continers within the pod
HostPath : A dummy one 

Persistent Volume states 

Available 
 A free resource that is not yet beyon to a claim 

 Bound : The volume is bound to a claim 

Released 
THe claim has been deleted, but the resources is not yet reclaimed by the cluster

Failed 

k apply -f [definition.yml]  Deploy the PVs and PVCs 
k get pv    get the pv list 
k get pvc   : Get the pvc list 
k describe pv [pvName] : : Describe the PV 
k describe pvc [pvcName] : : Describe the PVC 
k delete -f [definition.yml]  : Delete the PVs and PVCs 
k delete pv [pvName] : :  Delete the pv using it's name 
k delete pvc [pvcName] : : '" pvc " 



: : : : : : Dynamic Way : : : : : : 


------------configMaps - ----------- ----            

    Secrets 
> used to store config values in base64 string 
> 

k create secret generic [secretName] --from-literal=STATE=Michigan   
k apply -f [secret.name]   The decalrative way 
k get secrets : List of secrets 
k get secrets [secretName] -o YAML   : Save a secret in a YAML file 
k delete -f [secret.yaml]   Delete 
k get secrets [secretName]  

kubernetes dashboard 

> > > > > > > > > k9s > > > > >  > > ???????

^^^^^ ^^ ^^^^^ ^^ ^^^^^ ^^^ ^^^^^ ^^ ^^^^^ ^^^^ 
   Scaling pods 

Horizonatl pod autoscaling 

 Uses the k8s MEtrics server 
 Pods must have the requests and limits defined 
 The HPA check the metircs Server every 30 seconds 

 Scale according to the min and max number or replica defined 
 Cooldown / Delay 
  Prevent racing conditions
  Once a chnage has been made , HPA waits 
  By default the dalay on scale up events is 3 minutes and the delay on the scale down events is 5 mints 

-------------------------------------------------------------------------------------------------------

HPCA Cheat sheet 

k autoscale deplyments [name] --cpu-precent --min=3 --max=10   The imperative way 
k apply -f [hap.yml]                   > > > the decalrativ eway 
k get hpa [name]                       > > > Get the autoscaler status 
k delete -f [hap.yaml]                  > > delete the HPA 
k delete hpa [name]                     >  > > delete the HPA 
 
